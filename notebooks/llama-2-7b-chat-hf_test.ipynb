{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb33511-b53c-4324-885a-a12b4a3bccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b0edf9-8914-47f0-a1a0-6953d2db72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"home/drifter/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cef2261-6efb-4ee8-9d6b-b86502856723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializando el tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84f3baa-b6d4-40f7-b71a-1c9ca9eacf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7859763b-5b74-4ba5-925f-588d7c1a5a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28a856ebeb446299aa86acbcceb5cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_causal = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf417bb1-6457-4729-b6a3-dff99fba9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hola, como estas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13faec2-93a6-4595-91f3-ecd0fc93d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e970a4f-6676-4a50-8025-83786aa40b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# GENERAR RESPUESTA\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m model_causal\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# GENERAR RESPUESTA\n",
    "\n",
    "generate_ids = model_causal.generate(inputs.input_ids, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576fe175-0bf7-436d-bbb9-b3e3db920faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b98bdf-4056-4d1b-96a5-9ba5f56e22e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, como estas? 😊\\n\\nEste es un ejemplo de un mensaje en español, con un saludo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c312ae71-3f08-4519-82c6-055262362dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    system = \"Eres un asistente virtual que responde en el lenguaje en el que te escriben\"\n",
    "\n",
    "    prompt = system + \"\\n\" + prompt\n",
    "    # GENERAR RESPUESTA\n",
    "    generate_ids = model_causal.generate(inputs.input_ids)\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23305d37-987e-4d53-80e9-a5cc05d7e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def chat(prompt: str, lan: str = \"español\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    system = f\"Eres un asistente virtual que responde en {lan}\"\n",
    "\n",
    "    prompt = system + \"\\n\" + prompt\n",
    "    # GENERAR RESPUESTA\n",
    "    generate_ids = model_causal.generate(inputs.input_ids, max_length = 500)\n",
    "    response_tokens = generate_ids[0].tolist()\n",
    "    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    response = \"\"\n",
    "    for char in response_text:\n",
    "        response += char\n",
    "        # Imprimir letra por letra o palabra por palabra\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(0.025)  # Puedes ajustar el tiempo de pausa según tus preferencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8058a5c9-b7a8-4e36-a751-1008b6dfd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 16:32:52.068034: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 16:32:52.457438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 16:32:53.614934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cual es la diferencia entre chatgpt y llama 2 7b?\n",
      "\n",
      "ChatGPT es un modelo de lenguaje AI desarrollado por Meta AI que utiliza una arquitectura de modelo de lenguaje transformer para procesar texto y generar respuestas coherentes y naturales. ChatGPT tiene la capacidad de responder a preguntas y realizar tareas de conversación de manera efectiva, ya sea en contextos simples o complejos.\n",
      "LLaMA 2.7B es un modelo de lenguaje AI desarrollado por Facebook AI que utiliza una arquitectura de modelo de lenguaje transformer similar a la de ChatGPT. Sin embargo, LLaMA 2.7B tiene una capacidad de procesamiento de texto significativamente mayor que ChatGPT, lo que le permite manejar textos más largos y complejos de manera más eficiente.\n",
      "En términos de performance, ChatGPT y LLaMA 2.7B tienen resultados similares en tareas de conversación simples, pero LLaMA 2.7B tiene un rendimiento mejorado en tareas de conversación más complejas. Además, LLaMA 2.7B tiene una mayor capacidad de generación de texto, lo que significa que puede generar texto más largo y detallado de manera más eficiente que ChatGPT.\n",
      "En resumen, ChatGPT y LLaMA 2.7B son dos modelos de lenguaje AI muy poderosos que comparten ciertas similitudes, pero también tienen diferencias significativas en términos de capacidad de procesamiento, rendimiento y capacidad de generación de texto."
     ]
    }
   ],
   "source": [
    "chat(\"¿Cual es la diferencia entre chatgpt y llama 2 7b?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f95d-b73c-4a73-9013-7526adb3f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
