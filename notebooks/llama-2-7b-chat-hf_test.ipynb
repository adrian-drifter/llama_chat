{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb33511-b53c-4324-885a-a12b4a3bccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b0edf9-8914-47f0-a1a0-6953d2db72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"home/drifter/models/Llama-2-7b-chat-hf/ggml-model-f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cef2261-6efb-4ee8-9d6b-b86502856723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializando el tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84f3baa-b6d4-40f7-b71a-1c9ca9eacf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7859763b-5b74-4ba5-925f-588d7c1a5a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28a856ebeb446299aa86acbcceb5cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_causal = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf417bb1-6457-4729-b6a3-dff99fba9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hola, como estas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13faec2-93a6-4595-91f3-ecd0fc93d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e970a4f-6676-4a50-8025-83786aa40b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# GENERAR RESPUESTA\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m model_causal\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# GENERAR RESPUESTA\n",
    "\n",
    "generate_ids = model_causal.generate(inputs.input_ids, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576fe175-0bf7-436d-bbb9-b3e3db920faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b98bdf-4056-4d1b-96a5-9ba5f56e22e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, como estas? 游땕\\n\\nEste es un ejemplo de un mensaje en espa침ol, con un saludo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c312ae71-3f08-4519-82c6-055262362dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    system = \"Eres un asistente virtual que responde en el lenguaje en el que te escriben\"\n",
    "\n",
    "    prompt = system + \"\\n\" + prompt\n",
    "    # GENERAR RESPUESTA\n",
    "    generate_ids = model_causal.generate(inputs.input_ids)\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23305d37-987e-4d53-80e9-a5cc05d7e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def chat(prompt: str, lan: str = \"espa침ol\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    system = f\"Eres un asistente virtual que responde en {lan}\"\n",
    "\n",
    "    prompt = system + \"\\n\" + prompt\n",
    "    # GENERAR RESPUESTA\n",
    "    generate_ids = model_causal.generate(inputs.input_ids, max_length = 500)\n",
    "    response_tokens = generate_ids[0].tolist()\n",
    "    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    response = \"\"\n",
    "    for char in response_text:\n",
    "        response += char\n",
    "        # Imprimir letra por letra o palabra por palabra\n",
    "        print(char, end='', flush=True)\n",
    "        time.sleep(0.025)  # Puedes ajustar el tiempo de pausa seg칰n tus preferencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8058a5c9-b7a8-4e36-a751-1008b6dfd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 16:32:52.068034: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 16:32:52.457438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 16:32:53.614934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쮺ual es la diferencia entre chatgpt y llama 2 7b?\n",
      "\n",
      "ChatGPT es un modelo de lenguaje AI desarrollado por Meta AI que utiliza una arquitectura de modelo de lenguaje transformer para procesar texto y generar respuestas coherentes y naturales. ChatGPT tiene la capacidad de responder a preguntas y realizar tareas de conversaci칩n de manera efectiva, ya sea en contextos simples o complejos.\n",
      "LLaMA 2.7B es un modelo de lenguaje AI desarrollado por Facebook AI que utiliza una arquitectura de modelo de lenguaje transformer similar a la de ChatGPT. Sin embargo, LLaMA 2.7B tiene una capacidad de procesamiento de texto significativamente mayor que ChatGPT, lo que le permite manejar textos m치s largos y complejos de manera m치s eficiente.\n",
      "En t칠rminos de performance, ChatGPT y LLaMA 2.7B tienen resultados similares en tareas de conversaci칩n simples, pero LLaMA 2.7B tiene un rendimiento mejorado en tareas de conversaci칩n m치s complejas. Adem치s, LLaMA 2.7B tiene una mayor capacidad de generaci칩n de texto, lo que significa que puede generar texto m치s largo y detallado de manera m치s eficiente que ChatGPT.\n",
      "En resumen, ChatGPT y LLaMA 2.7B son dos modelos de lenguaje AI muy poderosos que comparten ciertas similitudes, pero tambi칠n tienen diferencias significativas en t칠rminos de capacidad de procesamiento, rendimiento y capacidad de generaci칩n de texto."
     ]
    }
   ],
   "source": [
    "chat(\"쮺ual es la diferencia entre chatgpt y llama 2 7b?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f95d-b73c-4a73-9013-7526adb3f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
